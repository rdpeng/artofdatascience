# Stating and Refining the Question {#chapter-question}

Doing data analysis requires quite a bit of thinking and we believe that when you’ve completed a good data analysis, you’ve spent more time thinking than doing. The thinking begins before you even look at a dataset, and it’s well worth devoting careful thought to your question.  This point cannot be over-emphasized as many of the “fatal” pitfalls of a data analysis can be avoided by expending the mental energy to get your question right.  In this chapter, we will discuss the characteristics of a good question, the types of questions that can be asked, and how to apply the iterative epicyclic process to stating and refining your question so that when you start looking at data, you have a sharp, answerable question.

## Types of Questions

Before we delve into stating the question, it's helpful to consider what the different types of questions are. There are six basic types of questions and much of the discussion that follows comes from a [paper](http://www.sciencemag.org/content/347/6228/1314.short) published in *Science* by Roger and [Jeff Leek](http://jtleek.com).  Understanding the type of question you are asking may be the most fundamental step you can take to ensure that, in the end, your interpretation of the results is correct.  The six types of questions are:

1.  Descriptive
2.  Exploratory
3.  Inferential
4.  Predictive
5.  Causal
6.  Mechanistic

And the type of question you are asking directly informs how you interpret your results.

A *descriptive* question is one that seeks to summarize a characteristic of a set of data.  Examples include determining the proportion of males, the mean number of servings of fresh fruits and vegetables per day, or the frequency of viral illnesses in a set of data collected from a group of individuals. There is no interpretation of the result itself as the result is a fact, an attribute of the set of data that you are working with.

An *exploratory* question is one in which you analyze the data to see if there are patterns, trends, or relationships between variables.  These types of analyses are also called “hypothesis-generating” analyses because rather than testing a hypothesis as would be done with an inferential, causal, or mechanistic question, you are looking for patterns that would support proposing a hypothesis.  If you had a general thought that diet was linked somehow to viral illnesses, you might explore this idea by examining relationships between a range of dietary factors and viral illnesses.  You find in your exploratory analysis that individuals who ate a diet high in certain foods had fewer viral illnesses than those whose diet was not enriched for these foods, so you propose the hypothesis that among adults, eating at least 5 servings a day of fresh fruit and vegetables is associated with fewer viral illnesses per year.

An *inferential* question would be a restatement of this proposed hypothesis as a question and would be answered by analyzing a different set of data, which in this example, is a representative sample of adults in the US.  By analyzing this different set of data you are both determining if the association you observed in your exploratory analysis holds in a different sample and whether it holds in a sample that is representative of the adult US population, which would suggest that the association is applicable to all adults in the US.  In other words, you will be able to infer what is true, on average, for the adult population in the US from the analysis you perform on the representative sample.  

A *predictive* question would be one where you ask what types of people will eat a diet high in fresh fruits and vegetables during the next year.  In this type of question you are less interested in what causes someone to eat a certain diet, just what predicts whether someone will eat this certain diet.  For example, higher income may be one of the final set of predictors, and you may not know (or even care) why people with higher incomes are more likely to eat a diet high in fresh fruits and vegetables, but what is most important is that income is a factor that predicts this behavior.  
 
Although an inferential question might tell us that people who eat a certain type of foods tend to have fewer viral illnesses, the answer to this question does not tell us if eating these foods causes a reduction in the number of viral illnesses, which would be the case for a *causal* question.  A causal question asks about whether changing one factor will change another factor, on average, in a population.  Sometimes the underlying design of the data collection, by default, allows for the question that you ask to be causal.  An example of this would be data collected in the context of a randomized trial, in which people were randomly assigned to eat a diet high in fresh fruits and vegetables or one that was low in fresh fruits and vegetables.  In other instances, even if your data are not from a randomized trial, you can take an analytic approach designed to answer a causal question. 

Finally, none of the questions described so far will lead to an answer that will tell us, if the diet does, indeed, cause a reduction in the number of viral illnesses, *how* the diet leads to a reduction in the number of viral illnesses.  A question that asks how a diet high in fresh fruits and vegetables leads to a reduction in the number of viral illnesses would be a *mechanistic* question.

There are a couple of additional points about the types of questions that are important.  First, by necessity, many data analyses answer multiple types of questions. For example, if a data analysis aims to answer an inferential question, descriptive and exploratory questions must also be answered during the process of answering the inferential question.  To continue our example of diet and viral illnesses, you would not jump straight to a statistical model of the relationship between a diet high in fresh fruits and vegetables and the number of viral illnesses without having determined the frequency of this type of diet and viral illnesses and their relationship to one another in this sample.  A second point is that the type of question you ask is determined in part by the data available to you (unless you plan to conduct a study and collect the data needed to do the analysis).   For example, you may want to ask a causal question about diet and viral illnesses to know whether eating a diet high in fresh fruits and vegetables causes a decrease in the number of viral illnesses, and the best type of data to answer this causal question is one in which people’s diets change from one that is high in fresh fruits and vegetables to one that is not, or vice versa.  If this type of data set does not exist, then the best you may be able to do is either apply causal analysis methods to observational data or instead answer an inferential question about diet and viral illnesses.


## Applying the Epicycle to Stating and Refining Your Question

You can now use the information about the types of questions and characteristics of good questions as a guide to refining your question.  To accomplish this, you can iterate through the 3 steps of:

1. Establishing your expectations about the question

2. Gathering information about your question

3. Determining if your expectations match the information you gathered, and then refining your question (or expectations) if your expecations did not match the information you gathered

## Characteristics of a Good Question

There are five key characteristics of a good question for a data analysis, which range from the very basic characteristic that the question should not have already been answered to the more abstract characteristic that each of the possible answers to the question should have a single interpretation and be meaningful.  We will discuss how to assess this in greater detail below. 

As a start, the question should be of **interest** to your audience, the identity of which will depend on the context and environment in which you are working with data.  If you are in academia, the audience may be your collaborators, the scientific community, government regulators, your funders, and/or the public.  If you are working at a start-up, your audience is your boss, the company leadership, and the investors.  As an example, answering the question of whether outdoor particulate matter pollution is associated with developmental problems in children may be of interest to people involved in regulating air pollution, but may not be of interest to a grocery store chain.  On the other hand, answering the question of whether sales of pepperoni are higher when it is displayed next to the pizza sauce and pizza crust or when it is displayed with the other packaged meats would be of interest to a grocery store chain, but not to people in other industries.

You should also check that the question has **not already been answered.**  With the recent explosion of data, the growing amount of publicly available data, and the seemingly endless scientific literature and other resources, it is not uncommon to discover that your question of interest has been answered already.  Some research and discussion with experts can help sort this out, and can also be helpful because even if the specific question you have in mind has not been answered, related questions may have been answered and the answers to these related questions are informative for deciding if or how you proceed with your specific question.

The question should also stem from a **plausible** framework.  In other words, the question above about the relationship between sales of pepperoni and its placement in the store is a plausible one because shoppers buying pizza ingredients are more likely than other shoppers to be interested in pepperoni and may be more likely to buy it if they see it at the same time that they are selecting the other pizza ingredients.  A less plausible question would be whether pepperoni sales correlate with yogurt sales, unless you had some prior knowledge suggesting that these should be correlated. 

If you ask a question whose framework is not plausible, you are likely to end up with an answer that's difficult to interpret or have confidence in.  In the pepperoni-yogurt question, if you do find they are correlated, many questions are raised about the result itself:  is it really correct?, why are these things correlated- is there another explanation?, and others.  You can ensure that your question is grounded in a plausible framework by using your own knowledge of the subject area and doing a little research, which together can go a long way in terms of helping you sort out whether your question is grounded in a plausible framework. 

The question, should also, of course, be **answerable**.  Although perhaps this doesn't need stating, it's worth pointing out that some of the best questions aren't answerable - either because the data don't exist or there is no means of collecting the data because of lack of resources, feasibility, or ethical problems.  For example, it is quite plausible that there are defects in the functioning of certain cells in the brain that cause autism, but it not possible to perform brain biopsies to collect live cells to study, which would be needed to answer this question.

**Specificity** is also an important characteristic of a good question.  An example of a general question is: Is eating a healthier diet better for you?  Working towards specificity will refine your question and directly inform what steps to take when you start looking at data.  A more specific question emerges after asking yourself what you mean by a "healthier" diet and when you say something is "better for you"?  The process of increasing the specificity should lead to a final, refined question such as:  "Does eating at least 5 servings per day of fresh fruits and vegetables lead to fewer upper respiratory tract infections (colds)?"  With this degree of specificity, your plan of attack is much clearer and the answer you will get at the end of the data analysis will be more interpretable as you will either recommend or not recommend the specific action of eating at least 5 servings of fresh fruit and vegetables per day as a means of protecting against upper respiratory tract infections.


## Translating a Question into a Data Problem

Another aspect to consider when you're developing your question is what will happen when you translate it into a data problem.  Every question must be operationalized as a data analysis that leads to a result. Pausing to think through what the results of the data analysis would look like and how they might be interpreted is important as it can prevent you from wasting a lot of time embarking on an analysis whose result is not interpretable. Although we will discuss many examples of questions that lead to interpretable and meaningful results throughout the book, it may be easiest to start first by thinking about what sorts of questions *don't* lead to interpretable answers.  

The typical type of question that does not meet this criterion is a question that uses inappropriate data.  For example, your question may be whether taking a vitamin D supplement is associated with fewer headaches, and you plan on answering that question by using the number of times a person took a pain reliever as a marker of the number of headaches they had.  You may find an association between taking vitamin D supplements and taking less pain reliever medication, but it won't be clear what the interpretation of this result is.  In fact, it is possible that people who take vitamin D supplements also tend to be less likely to take other over-the-counter medications just because they are "medication avoidant," and not because they are actually getting fewer headaches.  It may also be that they are using less pain reliever medication because they have less joint pain, or other types of pain, but not fewer headaches. Another interpretation, of course, is that they are indeed having fewer headaches, but the problem is that you can't determine whether this is the correct interpretation or one of the other interpretations is correct.  In essence, the problem with this question is that for a single possible answer, there are multiple interpretations.  This scenario of multiple interpretations arises when at least one of the variables you use (in this case, pain reliever use) is not a good measure of the concept you are truly after (in this case, headaches).  To head off this problem, you will want to make sure that the data available to answer your question provide reasonably specific measures of the factors required to answer your question.

A related problem that interferes with interpretation of results is confounding.  Confounding is a potential problem when your question asks about the relationship between factors, such as taking vitamin D and frequency of headaches.  A brief description of the concept of confounding is that it is present when a factor that you were not necessarily considering in your question is related to both your exposure of interest (in the example, taking vitamin D supplements) and your outcome of interest (taking pain reliever medication).  For example, income could be a confounder, because it may be related to both taking vitamin D supplements and frequency of headaches, since people with higher income may tend to be more likely to take a supplement and less likely to have chronic health problems, such as headaches.  Generally, as long as you have income data available to you, you will be able to adjust for this confounder and reduce the number of possible interpretations of the answer to your question.  As you refine your question, spend some time identifying the potential confounders and thinking about whether your dataset includes information about these potential confounders.

Another type of problem that can occur when inappropriate data are used is that the result is not interpretable because the underlying way in which the data were collected lead to a biased result.  For example, imagine that you are using a dataset created from a survey of women who had had children.  The survey includes information about whether their children had autism and whether they reported eating sushi while pregnant, and you see an association between report of eating sushi during pregnancy and having a child with autism.  However, because women who have had a child with a health condition recall the exposures, such as raw fish, that occurred during pregnancy differently than those who have had healthy children, the observed association between sushi exposure and autism may just be the manifestation of a mother's tendency to focus more events during pregnancy when she has a child with a health condition.  This is an example of recall bias, but there are many types of bias that can occur.  

The other major bias to understand and consider when refining your question is selection bias, which occurs when the data your are analyzing were collected in such a way to inflate the proportion of people who have both characteristics above what exists in the general population.  If a study advertised that it was a study about autism and diet during pregnancy, then it is qute possible that women who both ate raw fish and had a child with autism would be more likely to respond to the survey than those who had one of these conditions or neither of these conditions.  This scenario would lead to a biased answer to your question about mothers' sushi intakes during pregnancy and risk of autism in their children.  A good rule of thumb is that if you are examining relationships between two factors, bias may be a problem if you are more (or less) likely to observe individuals with both factors because of how the population was selected, or how a person might recall the past when responding to a survey.  There will be more discussion about bias in subsequent chapters on ([Inference: A Primer](#chapter-inference) and [Interpreting Your Results](#chapter-interpretation)), but the best time to consider its effects on your data analysis is when you are identifying the question you will answer and thinking about how you are going to answer the question with the data available to you.

## Case Study

Joe works for a company that makes a variety of fitness tracking devices and apps and the name of the company is Fit on Fleek.  Fit on Fleek's goal is, like many tech start-ups, to use the data they collect from users of their devices to do targeted marketing of various products.  The product that they would like to market is a new one that they have just developed and not yet started selling, which is a sleep tracker and app that tracks various phases of sleep, such as REM sleep, and also provides advice for improving sleep. The sleep tracker is called Sleep on Fleek.  

Joe's boss asks him to analyze the data that the company has on its users of their health tracking devices and apps to identify users for targeted Sleep on Fleek ads.  Fit on Fleek has the following data from each of their customers: basic demographic information, number of steps walked per day, number of flights of stairs climbed per day, sedentary awake hours per day, hours of alertness per day, hours of drowsiness per day, and hours slept per day (but not more detailed information about sleep that the sleep tracker would track).

Although Joe has an objective in mind, gleaned from a discussion with his boss, and he also knows what types of data are available in the Fit on Fleek database, he does not yet have a question.  This scenario, in which Joe is given an objective, but not a question, is common, so Joe's first task is to translate the objective into a question, and this will take some back-and-forth communication with his boss.  The approach to informal communications that take place during the process of the data analysis project, is covered in detail in the [Communication chapter](#chapter-communication).  After a few discussions, Joe settles on the following question: "Which Fit on Fleek users don't get enough sleep?"  He and his boss agree that the customers who would be most likely to be interested in purchasing the Sleep on Fleek device and app are those who appear to have problems with sleep, and the easiest problem to track and probably the most common problem is not getting enough sleep. 

You might think that since Joe now has a question, that he should move to download the data and start doing exploratory analyses, but there is a bit of work Joe still has to do to refine the question.  The two main tasks Joe needs to tackle are: (1) to think through how his question does, or does not, meet the characteristics of a good question and (2) to determine what type of question he is asking so that he has a good understanding of what kinds of conclusions can (and cannot) be drawn when he has finished the data analysis.

Joe reviews the characteristics of a good question and his expecations are that his question has all of these characteristics:
-of interest
-not already answered
-grounded in a plausible framework
-answerable
-specific

The answer that he will get at the end of his analysis (when he translates his question into a data problem) should also be interpretable.

He then thinks through what he knows about the question and in his judgment, the question is of interest as his boss expressed interest.  

He also knows that the question could not have been answered already since his boss indicated that it had not and a review of the company's previous data analyses reveals no previous analysis designed to answer the question. 

Next he assesses whether the question is grounded in a plausible framework.  The question, Which Fit on Fleek users don't get enough sleep?, seems to be grounded in plausibility as it makes sense that people who get too little sleep would be interested in trying to improve their sleep by tracking it.  However, Joe wonders whether the duration of sleep is the best marker for whether a person feels that they are getting inadequate sleep.  He knows some people who regularly get little more than 5 hours of sleep a night and they seem satisfied with their sleep.  Joe reaches out to a sleep medicine specialist and learns that a better measure of whether someone is affected by lack of sleep or poor quality sleep is daytime drowsiness.  It turns out that his initial expectation that the question was grounded in a plausible framework did not match the information he received when he spoke with a content expert.  So he revises his question so that it matches his expectations of plausibility and the revised question is: Which Fit on Fleek users have drowsiness during the day?

Joe pauses to make sure that this question is, indeed, answerable with the data he has available to him, and confirms that it is.  He also pauses to think about the specificity of the question.  He believes that it is specific, but goes through the exercise of discussing the question with colleagues to gather information about the specificity of the question.  When he raises the idea of answering this question, his colleagues ask him many questions about what various parts of the question mean:  what is meant by "which users"?  Does this mean: What are the demographic characteristics of the users who have drowsiness?  Or something else?  What about "drowsiness during the day"?  Should this phrase mean any drowsiness on any day? Or drowsiness lasting at least a certain amount of time on at least a certain number of days?  The conversation with colleagues was very informative and indicated that the question was not very specific.  Joe revises his question so that it is now specific: "Which demographic and health characteristics identify users who are most likely to have chronic drowsiness, defined as at least one episode of drowsiness at least every other day?"

Joe now moves on to thinking about what the possible answers to his questions are, and whether they will be interpretable.  Joe identifies two possible outcomes of his analysis: (1)  there are no characteristics that identify people who have chronic daytime drowsiness or (2) there are one or more characteristics that identify people with chronic daytime drowsiness.  These two possibilities are interpretable and meaningful.  For the first, Joe would conclude that targeting ads for the Sleep on Fleek tracker to people who are predicted to have chronic daytime drowsiness would not be possible, and for the second, he'd conclude that targeting the ad is possible, and he'd know which characteristic(s) to use to select people for the targeted ads.

Now that Joe has a good question in hand, after iterating through the 3 steps of the epicycle as he considered whether his question met each of the characteristics of a good question, the next step is for him to figure out what type of question he has.  He goes through a thought process similar to the process he used for each of the characteristics above.  He starts thinking that his question is an exploratory one, but as he reviews the description and examples of an exploratory question, he realizes that although some parts of the analysis he will do to answer the question will be exploratory, ultimately his question is more than exploratory because its answer will predict which users are likely to have chronic daytime drowsiness, so his question is a prediction question.  Identifing the type of question is very helpful because, along with a good question, he now knows that he needs to use a prediction approach in his analyses, in particular in the model building phase (see [Formal Modeling chapter](#chapter-formal)).


## Concluding Thoughts

By now, you should be poised to apply the 3 steps of the epicycle to stating and refining a question. If you are a seasoned data analyst, much of this process may be automatic, so that you may not be entirely conscious of some parts of the process that lead you to a good question. Until you arrive at this point, this chapter can serve as a useful resource to you when you're faced with the task of developing a good question.  In the next chapters, we will discuss what to do with the data now that you have good question in hand. 
